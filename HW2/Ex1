############################################################################ Importing Liberaries ############################################################################
import argparse
import numpy as np
import os
import pandas as pd
import tensorflow as tf

from tensorflow import keras


# parser = argparse.ArgumentParser()
# parser.add_argument('--version', type=int, required=True, help='version --> can be ["a" , "b"]')
# args = parser.parse_args()

seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)

zip_path = tf.keras.utils.get_file(
    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',
    fname='jena_climate_2009_2016.csv.zip',
    extract=True,
    cache_dir='.', cache_subdir='data')
csv_path, _ = os.path.splitext(zip_path)
df = pd.read_csv(csv_path)

column_indices = [2, 5]
columns = df.columns[column_indices]
data = df[columns].values.astype(np.float32)

n = len(data)
train_data = data[0:int(n*0.7)]
val_data = data[int(n*0.7):int(n*0.9)]
test_data = data[int(n*0.9):]

mean = train_data.mean(axis=0)
std = train_data.std(axis=0)


print (mean.shape)

# WindowGenerator

input_width = 6
version = args.version                   
if version == "a" :               
    output_steps = 3
if version == "b" :
    output_steps = 9

############################################################################ Create the WindowGenerator Class #######################################################

class WindowGenerator:
    def __init__(self, input_width, output_steps, mean, std):
        self.input_width = input_width
        self.output_steps = output_steps
        self.mean = tf.reshape(tf.convert_to_tensor(mean), [1, 1, 2])
        self.std = tf.reshape(tf.convert_to_tensor(std), [1, 1, 2])


    def split_window(self, features):
        inputs = features[:, :self.input_width, :]        # for example if total window size = 9 input =  [:,:6 ,:] --> output [:,-3: , ] outpu_tstep = 3 
        labels = features[:, -self.output_steps :, :]

        inputs.set_shape([None, self.input_width, 2])
        labels.set_shape([None, self.output_steps,2])

        return inputs, labels

    def normalize(self, features):
        features = (features - self.mean) / (self.std + 1.e-6)

        return features

    def preprocess(self, features):
        inputs, labels = self.split_window(features)
        inputs = self.normalize(inputs)

        return inputs, labels

    def make_dataset(self, data, train):
        ds = tf.keras.preprocessing.timeseries_dataset_from_array(
                data=data,
                targets=None,
                sequence_length = input_width + self.output_steps,                       #### this change because now the total depends on the output widht
                sequence_stride = 1,
                batch_size = 32)
        ds = ds.map(self.preprocess)
        ds = ds.cache()
        if train is True:
            ds = ds.shuffle(100, reshuffle_each_iteration=True)

        return ds

############################################################################ Create datasets ############################################################################

generator = WindowGenerator(input_width, output_steps, mean, std)
train_ds = generator.make_dataset(train_data, True)
val_ds = generator.make_dataset(val_data, False)
test_ds = generator.make_dataset(test_data, False)

############################################################################ checking the shapes ############################################################################

print(f"data shape before split {data.shape}")

print(f"train_data {train_data.shape}")

print(f"val_data {val_data.shape}")

print(f"test_data {test_data.shape}")



it = iter(train_ds)


inp , label = next(it)
print(inp.shape)
print(label.shape)


############################################################################ Ex 1.7 MultiOutputMAE Metric ############################################################################
class MultiOutputMAE(tf.keras.metrics.Metric):
    def __init__(self, name='mean_absolute_error', **kwargs):
        super().__init__(name=name, **kwargs)
        self.total = self.add_weight('total', initializer='zeros', shape=(2,))
        self.count = self.add_weight('count', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        error = tf.abs(y_pred- y_true)     # change here y_pred.reshape(y_true.shape)
        error = tf.reduce_mean(error, axis=[0,1])
        self.total.assign_add(error)
        self.count.assign_add(1.)

        return

    def reset_states(self):
        self.count.assign(tf.zeros_like(self.count))
        self.total.assign(tf.zeros_like(self.total))

    def result(self):
        result = tf.math.divide_no_nan(self.total, self.count)

        return result



############################################################################ building the models ############################################################################

# Exercise 1.3
mlp = tf.keras.Sequential([
        tf.keras.layers.Flatten(input_shape = (input_width,2) , name='Flatten'),
        tf.keras.layers.Dense(128, activation='relu' , name='Dense1'),
        tf.keras.layers.Dense(128, activation='relu' , name='Dense2'),
        tf.keras.layers.Dense(units = 2*output_steps , name='Output_layes'), 
        tf.keras.layers.Reshape([output_steps, 2])
    ])

############################################################################
cnn = tf.keras.Sequential([
        tf.keras.layers.Conv1D(input_shape = (input_width,2) , filters=64, kernel_size=3, activation='relu'),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units=64, activation='relu'),
        tf.keras.layers.Dense(units=2*output_steps), 
        tf.keras.layers.Reshape([output_steps, 2])
    ])

############################################################################

lstm = tf.keras.Sequential([
        tf.keras.layers.LSTM(input_shape = (input_width,2) ,units=64),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units=2*output_steps),
        tf.keras.layers.Reshape([output_steps, 2]), 
    ])

MODELS = {'mlp': mlp, 'cnn': cnn, 'lstm': lstm}
model = MODELS['lstm']  #

# ! pip install pydot
# ! pip install pydotplus
# ! pip install graphviz
# print(model.summary())

import pydot
tf.keras.utils.plot_model(model, to_file='model.png',show_layer_names=True , show_shapes = True)

# Define losses & Optimizer & metrics

loss =   tf.keras.losses.MeanSquaredError()                       #tf.keras.losses.MeanSquaredError()
optimizer = tf.keras.optimizers.Adam()
metrics = [MultiOutputMAE()]

# Training and optimizing

model.compile(loss = loss, optimizer = optimizer, metrics = metrics)

tb_run = 0
tb_callback = keras.callbacks.TensorBoard(log_dir='./tb_log/run_{}'.format(tb_run), histogram_freq=1)
#history = model.fit(inp_train,target_train,  epochs=5,validation_data=val_ds ,batch_size=len(batch_train), callbacks=[tb_callback])
model.fit(train_ds, epochs=20,   validation_data=val_ds,callbacks=[tb_callback])
tb_run += 1

print(model.summary())

# How to change to multi output labels 

# x = np.arange(18)

# x = x.reshape((9,2))

# print(x.shape)
# print(x)
# print(x.mean(axis=0))



# print(x[:6,:].shape)
# print(x[:6,:])

# print(x[-3:,:].shape)
# print(x[-3:,:])

# train_ds.element_spec

# how to change the multi output Mean Square Error

# # Multi 
# x=np.arange(3*2).reshape(3,2)

# print(x)

# y= np.arange(6)[::-1].reshape(x.shape)

# # y.reshape(3,2)
# print(y)

# z = np.abs(y-x)

# print(np.mean(z,axis=0))

