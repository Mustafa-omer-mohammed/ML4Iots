{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e854e63-b2d1-4066-9018-5e471fbdcaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e854e63-b2d1-4066-9018-5e471fbdcaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.lite as tflite\n",
    "from tensorflow import keras\n",
    "import zlib\n",
    "\n",
    "import tensorflow_model_optimization as tfmot   \n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, required=True, help='model name')\n",
    "# parser.add_argument('--labels', type=int, required=True, help='model output')\n",
    "# args = parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb58f23-5c70-495d-9a19-2d746ca6ecc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.11\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2630fca0-5061-4117-a6d9-1a36ca4a4e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True,\n",
    "    cache_dir='.', cache_subdir='data')\n",
    "csv_path, _ = os.path.splitext(zip_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "column_indices = [2, 5]\n",
    "columns = df.columns[column_indices]\n",
    "data = df[columns].values.astype(np.float32)\n",
    "\n",
    "n = len(data)\n",
    "train_data = data[0:int(n*0.7)]\n",
    "val_data = data[int(n*0.7):int(n*0.9)]\n",
    "test_data = data[int(n*0.9):]\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "\n",
    "print (mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2997a3e-9d6e-49c8-bb17-6469e0602568",
   "metadata": {},
   "source": [
    "# WindowGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f0113e-bd14-4dd6-9872-c045adf11913",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = 6\n",
    "version = \"a\"                     ####### Remove this later\n",
    "if version == \"a\" :               ####### Remember this is the argparse Arument --version \n",
    "    output_steps = 3\n",
    "if version == \"b\" :\n",
    "    output_steps = 9\n",
    "\n",
    "\n",
    "class WindowGenerator:\n",
    "    def __init__(self, input_width, output_steps, mean, std):\n",
    "        self.input_width = input_width\n",
    "        self.output_steps = output_steps\n",
    "        self.mean = tf.reshape(tf.convert_to_tensor(mean), [1, 1, 2])\n",
    "        self.std = tf.reshape(tf.convert_to_tensor(std), [1, 1, 2])\n",
    "\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, :self.input_width, :]        # for example if total window size = 9 input =  [:,:6 ,:] --> output [:,-3: , ] outpu_tstep = 3 \n",
    "        labels = features[:, -self.output_steps :, :]\n",
    "\n",
    "        inputs.set_shape([None, self.input_width, 2])\n",
    "        labels.set_shape([None, self.output_steps,2])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def normalize(self, features):\n",
    "        features = (features - self.mean) / (self.std + 1.e-6)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def preprocess(self, features):\n",
    "        inputs, labels = self.split_window(features)\n",
    "        inputs = self.normalize(inputs)\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data, train):\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "                data=data,\n",
    "                targets=None,\n",
    "                sequence_length = input_width + self.output_steps,                       #### this change because now the total depends on the output widht\n",
    "                sequence_stride = 1,\n",
    "                batch_size = 32)\n",
    "        ds = ds.map(self.preprocess)\n",
    "        ds = ds.cache()\n",
    "        if train is True:\n",
    "            ds = ds.shuffle(100, reshuffle_each_iteration=True)\n",
    "\n",
    "        return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669e1a5f-8f69-45c8-8113-a5f9d2c58ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = np.arange(18)\n",
    "# window = window.reshape(9,2)\n",
    "# print(len(window))\n",
    "# print(window)\n",
    "# print(window[:6].shape)\n",
    "# print(window[-3:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0494564-1658-4fef-954a-c68d8e43aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = WindowGenerator(input_width, output_steps, mean, std)\n",
    "train_ds = generator.make_dataset(train_data, True)\n",
    "val_ds = generator.make_dataset(val_data, False)\n",
    "test_ds = generator.make_dataset(test_data, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df990243-6f6c-423c-82ac-48394aaba1e0",
   "metadata": {},
   "source": [
    "# checking the shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43459a86-1f13-437e-97cb-d69cf31dd1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape before split (420551, 2)\n",
      "train_data (294385, 2)\n",
      "val_data (84110, 2)\n",
      "test_data (42056, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"data shape before split {data.shape}\")\n",
    "\n",
    "print(f\"train_data {train_data.shape}\")\n",
    "\n",
    "print(f\"val_data {val_data.shape}\")\n",
    "\n",
    "print(f\"test_data {test_data.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7674be-c965-434b-b755-6b580a570ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb242899-5272-42fa-8bbd-8a7549fc3e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.7288457  0.6097644], shape=(2,), dtype=float32)\n",
      "tf.Tensor([ 2.58 86.2 ], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inp , label = next(it)\n",
    "print(inp[0,0,:])\n",
    "print(label[0,0,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fec2d-cfbf-4d4b-83d1-c79d9990dfc1",
   "metadata": {},
   "source": [
    "# MultiOutputMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04dbeda9-86ed-4738-8ef3-00e95ef20e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exercise 1.7\n",
    "class MultiOutputMAE(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='mean_absolute_error', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total = self.add_weight('total', initializer='zeros', shape=(2,))\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        error = tf.abs(y_pred- y_true)     \n",
    "        error = tf.reduce_mean(error, axis=[0,1])  # compute the mean for all samples in the batch for each feature (temp , hum) ==> output shape = (2,)\n",
    "        self.total.assign_add(error)\n",
    "        self.count.assign_add(1.)\n",
    "\n",
    "        return\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.count.assign(tf.zeros_like(self.count))\n",
    "        self.total.assign(tf.zeros_like(self.total))\n",
    "\n",
    "    def result(self):\n",
    "        result = tf.math.divide_no_nan(self.total, self.count)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf89bc-9183-41d8-8b5a-5cbd15171774",
   "metadata": {},
   "source": [
    "# building the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79ec65fb-5e88-4598-b606-838136be3a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_V( a_alpha=0.03 )\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.03\n",
    "# alpha = 1\n",
    "sparsity = 0.9\n",
    "Structured = True\n",
    "if Structured == True :\n",
    "    model_version = f\"_V( {version}_alpha={alpha} )\"\n",
    "else :\n",
    "    model_version = f\"_V( {version}_Sparcity ={sparsity} )\"\n",
    "mymodel = 'mlp'+ model_version\n",
    "chk_path = f'./callback_{mymodel}_chkp/{mymodel}_chkp_best'     # path for saving the best model \n",
    "TFLITE = mymodel + \".tflite\"                                    # path for saving the best model after converted to TF.lite model \n",
    "\n",
    "print(mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10a7f2c8-643d-4fa1-9d3a-3f6ac383907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### Build models with Structured Pruning via width Multiplier #####################################################\n",
    "def bulid_models_Structured (alpha = alpha , version = version , input_width = input_width ,output_steps = output_steps ,model_version = model_version  ) :\n",
    "    mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape = (input_width,2) , name='Flatten'),\n",
    "            tf.keras.layers.Dense(int(128 *alpha), activation='relu' , name='Dense1'),\n",
    "            tf.keras.layers.Dense(int(128 *alpha), activation='relu' , name='Dense2'),\n",
    "            tf.keras.layers.Dense(units = 2*output_steps , name='Output_layes'), \n",
    "            tf.keras.layers.Reshape([output_steps, 2])\n",
    "        ])\n",
    "\n",
    "    ############################################################################\n",
    "    cnn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv1D(input_shape = (input_width,2) , filters=int(64 *alpha), kernel_size=3, activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(units=int(64 *alpha), activation='relu'),\n",
    "            tf.keras.layers.Dense(units=2*output_steps), \n",
    "            tf.keras.layers.Reshape([output_steps, 2])\n",
    "        ])\n",
    "\n",
    "  \n",
    "\n",
    "    MODELS = {'mlp'+ model_version: mlp, 'cnn'+ model_version: cnn }\n",
    "    return MODELS \n",
    "\n",
    "####################################       Build models with UnStructured Pruning         #####################################################\n",
    "\n",
    "def bulid_models_UnStructured ( version = version , input_width = input_width ,output_steps = output_steps ,model_version = model_version  ) :\n",
    "    mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape = (input_width,2) , name='Flatten'),\n",
    "            tf.keras.layers.Dense(128, activation='relu' , name='Dense1'),\n",
    "            tf.keras.layers.Dense(128, activation='relu' , name='Dense2'),\n",
    "            tf.keras.layers.Dense(units = 2*output_steps , name='Output_layes'), \n",
    "            tf.keras.layers.Reshape([output_steps, 2])\n",
    "        ])\n",
    "\n",
    "    ############################################################################\n",
    "    cnn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv1D(input_shape = (input_width,2) , filters=64, kernel_size=3, activation='relu' , name= \"Conv1D-1\"),\n",
    "            tf.keras.layers.Flatten(name='Flatten'),\n",
    "            tf.keras.layers.Dense(units=64, activation='relu', name='Dense-1'),\n",
    "            tf.keras.layers.Dense(units=2*output_steps), \n",
    "            tf.keras.layers.Reshape([output_steps, 2])\n",
    "        ])\n",
    "\n",
    " \n",
    "\n",
    "    MODELS = {'mlp'+ model_version: mlp, 'cnn'+ model_version: cnn }\n",
    "    return MODELS \n",
    "\n",
    "if Structured == True :\n",
    "    MODELS = bulid_models_Structured()\n",
    "else :\n",
    "    MODELS = bulid_models_UnStructured()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71ff7847-5c1f-4d91-b905-1df171f6139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pydot\n",
    "# ! pip install pydotplus\n",
    "# ! pip install graphviz\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aa53ad5-a5ef-459f-927d-207af18b47f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mlp_V( a_alpha=0.03 )', 'cnn_V( a_alpha=0.03 )'])\n"
     ]
    }
   ],
   "source": [
    "print(MODELS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81720020-46c5-4fa7-ba23-dcb757909f79",
   "metadata": {},
   "source": [
    "# Define losses & Optimizer & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6111f00-342d-4f04-8309-5b160a9d7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model = mymodel ):\n",
    "    model = MODELS[model]\n",
    "    loss =   tf.keras.losses.MeanSquaredError()                       #tf.keras.losses.MeanSquaredError()\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    metrics = [MultiOutputMAE()]\n",
    "\n",
    "    # Training and optimizing\n",
    "\n",
    "    model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8629de26-4691-43e6-ae6d-07a1311af0f0",
   "metadata": {},
   "source": [
    "# define call backs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d09677-3b03-4ad3-ad0a-1565915ebeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TEMP_HUM_VAL  callback to print the MAE for Temperature and humidity in more interpetable format \n",
    "class TEMP_HUM_VAL(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        hum = logs[\"val_mean_absolute_error\"][1]\n",
    "        temp = logs[\"val_mean_absolute_error\"][0]\n",
    "        MAE = logs[\"val_mean_absolute_error\"]\n",
    "        print(f\"\\n Temp MAE = {temp:.3f}, Hum MAE = {hum:.3f}    \")\n",
    "        # return temp , hum\n",
    "\n",
    "\n",
    "mycallback = TEMP_HUM_VAL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc272011-026f-4d88-a371-2e689288906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint callback to save the best model \n",
    "cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "    f'./callback_{mymodel}_chkp/{mymodel}_chkp_best',\n",
    "    # './callback_test_chkp/chkp_best',\n",
    "    monitor='val_loss',\n",
    "    verbose=0, \n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d96e2b24-943a-4fb5-8704-592d6d7d0aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9193/9200 [============================>.] - ETA: 0s - loss: 520.7838 - mean_absolute_error: 12.9874\n",
      " Temp MAE = 9.541, Hum MAE = 2.412    \n",
      "WARNING:tensorflow:From C:\\Users\\musta\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\musta\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 13s 1ms/step - loss: 520.4069 - mean_absolute_error: 12.9801 - val_loss: 68.3634 - val_mean_absolute_error: 5.9766\n",
      "Epoch 2/50\n",
      "9135/9200 [============================>.] - ETA: 0s - loss: 29.3626 - mean_absolute_error: 3.5652\n",
      " Temp MAE = 4.312, Hum MAE = 2.886    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 11s 1ms/step - loss: 29.2187 - mean_absolute_error: 3.5539 - val_loss: 22.9285 - val_mean_absolute_error: 3.5994\n",
      "Epoch 3/50\n",
      "9161/9200 [============================>.] - ETA: 0s - loss: 3.8271 - mean_absolute_error: 1.2104\n",
      " Temp MAE = 0.535, Hum MAE = 1.307    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 11s 1ms/step - loss: 3.8215 - mean_absolute_error: 1.2091 - val_loss: 2.4596 - val_mean_absolute_error: 0.9209\n",
      "Epoch 4/50\n",
      "9177/9200 [============================>.] - ETA: 0s - loss: 2.1079 - mean_absolute_error: 0.8042\n",
      " Temp MAE = 0.347, Hum MAE = 1.160    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 10s 1ms/step - loss: 2.1075 - mean_absolute_error: 0.8042 - val_loss: 2.0066 - val_mean_absolute_error: 0.7535\n",
      "Epoch 5/50\n",
      "9158/9200 [============================>.] - ETA: 0s - loss: 2.0008 - mean_absolute_error: 0.7674\n",
      " Temp MAE = 0.331, Hum MAE = 1.201    \n",
      "9200/9200 [==============================] - 9s 946us/step - loss: 2.0048 - mean_absolute_error: 0.7682 - val_loss: 2.0757 - val_mean_absolute_error: 0.7663\n",
      "Epoch 6/50\n",
      "9130/9200 [============================>.] - ETA: 0s - loss: 1.9653 - mean_absolute_error: 0.7526\n",
      " Temp MAE = 0.337, Hum MAE = 1.157    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 9s 1ms/step - loss: 1.9675 - mean_absolute_error: 0.7536 - val_loss: 1.9692 - val_mean_absolute_error: 0.7470\n",
      "Epoch 7/50\n",
      "9173/9200 [============================>.] - ETA: 0s - loss: 1.9369 - mean_absolute_error: 0.7407\n",
      " Temp MAE = 0.276, Hum MAE = 1.156    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 9s 996us/step - loss: 1.9402 - mean_absolute_error: 0.7414 - val_loss: 1.9470 - val_mean_absolute_error: 0.7159\n",
      "Epoch 8/50\n",
      "9131/9200 [============================>.] - ETA: 0s - loss: 1.9210 - mean_absolute_error: 0.7349- ETA: 0s - loss: 1.8891 - mean_absolute\n",
      " Temp MAE = 0.309, Hum MAE = 1.305    \n",
      "9200/9200 [==============================] - 8s 856us/step - loss: 1.9252 - mean_absolute_error: 0.7358 - val_loss: 2.0540 - val_mean_absolute_error: 0.8069\n",
      "Epoch 9/50\n",
      "9190/9200 [============================>.] - ETA: 0s - loss: 1.9131 - mean_absolute_error: 0.7295\n",
      " Temp MAE = 0.281, Hum MAE = 1.113    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 9s 998us/step - loss: 1.9130 - mean_absolute_error: 0.7296 - val_loss: 1.8648 - val_mean_absolute_error: 0.6972\n",
      "Epoch 10/50\n",
      "9194/9200 [============================>.] - ETA: 0s - loss: 1.9052 - mean_absolute_error: 0.7252\n",
      " Temp MAE = 0.258, Hum MAE = 1.097    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 10s 1ms/step - loss: 1.9053 - mean_absolute_error: 0.7253 - val_loss: 1.8607 - val_mean_absolute_error: 0.6775\n",
      "Epoch 11/50\n",
      "9173/9200 [============================>.] - ETA: 0s - loss: 1.8952 - mean_absolute_error: 0.7203\n",
      " Temp MAE = 0.262, Hum MAE = 1.141    \n",
      "9200/9200 [==============================] - 8s 892us/step - loss: 1.8956 - mean_absolute_error: 0.7206 - val_loss: 1.8764 - val_mean_absolute_error: 0.7015\n",
      "Epoch 12/50\n",
      "9187/9200 [============================>.] - ETA: 0s - loss: 1.8869 - mean_absolute_error: 0.717 - ETA: 0s - loss: 1.8890 - mean_absolute_error: 0.7185\n",
      " Temp MAE = 0.259, Hum MAE = 1.101    \n",
      "9200/9200 [==============================] - 9s 934us/step - loss: 1.8901 - mean_absolute_error: 0.7188 - val_loss: 1.8623 - val_mean_absolute_error: 0.6798\n",
      "Epoch 13/50\n",
      "9171/9200 [============================>.] - ETA: 0s - loss: 1.8887 - mean_absolute_error: 0.7178\n",
      " Temp MAE = 0.262, Hum MAE = 1.100    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 10s 1ms/step - loss: 1.8896 - mean_absolute_error: 0.7180 - val_loss: 1.8403 - val_mean_absolute_error: 0.6811\n",
      "Epoch 14/50\n",
      "9187/9200 [============================>.] - ETA: 0s - loss: 1.8891 - mean_absolute_error: 0.7170\n",
      " Temp MAE = 0.262, Hum MAE = 1.113    \n",
      "9200/9200 [==============================] - 10s 1ms/step - loss: 1.8886 - mean_absolute_error: 0.7170 - val_loss: 1.8655 - val_mean_absolute_error: 0.6875\n",
      "Epoch 15/50\n",
      "9167/9200 [============================>.] - ETA: 0s - loss: 1.8784 - mean_absolute_error: 0.7136- ETA: 0s - loss: 1.8779 - mean_absolute_error: 0.713\n",
      " Temp MAE = 0.299, Hum MAE = 1.341    \n",
      "9200/9200 [==============================] - 9s 1ms/step - loss: 1.8819 - mean_absolute_error: 0.7143 - val_loss: 2.0659 - val_mean_absolute_error: 0.8200\n",
      "Epoch 16/50\n",
      "9124/9200 [============================>.] - ETA: 0s - loss: 1.8814 - mean_absolute_error: 0.7137\n",
      " Temp MAE = 0.258, Hum MAE = 1.128    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 9s 1ms/step - loss: 1.8818 - mean_absolute_error: 0.7143 - val_loss: 1.8385 - val_mean_absolute_error: 0.6927\n",
      "Epoch 17/50\n",
      "9153/9200 [============================>.] - ETA: 0s - loss: 1.8791 - mean_absolute_error: 0.7140\n",
      " Temp MAE = 0.270, Hum MAE = 1.168    \n",
      "9200/9200 [==============================] - 8s 881us/step - loss: 1.8840 - mean_absolute_error: 0.7151 - val_loss: 1.8875 - val_mean_absolute_error: 0.7190\n",
      "Epoch 18/50\n",
      "9152/9200 [============================>.] - ETA: 0s - loss: 1.8776 - mean_absolute_error: 0.7127\n",
      " Temp MAE = 0.267, Hum MAE = 1.194    \n",
      "9200/9200 [==============================] - 8s 879us/step - loss: 1.8797 - mean_absolute_error: 0.7135 - val_loss: 1.9067 - val_mean_absolute_error: 0.7304\n",
      "Epoch 19/50\n",
      "9175/9200 [============================>.] - ETA: 0s - loss: 1.8757 - mean_absolute_error: 0.7130- ETA: 0s - loss: 1.8597 - mean_absolute_error: 0.7\n",
      " Temp MAE = 0.279, Hum MAE = 1.225    \n",
      "9200/9200 [==============================] - 9s 950us/step - loss: 1.8790 - mean_absolute_error: 0.7137 - val_loss: 1.9333 - val_mean_absolute_error: 0.7518\n",
      "Epoch 20/50\n",
      "9161/9200 [============================>.] - ETA: 0s - loss: 1.8771 - mean_absolute_error: 0.7134\n",
      " Temp MAE = 0.240, Hum MAE = 1.096    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 9s 960us/step - loss: 1.8812 - mean_absolute_error: 0.7138 - val_loss: 1.8271 - val_mean_absolute_error: 0.6679\n",
      "Epoch 21/50\n",
      "9196/9200 [============================>.] - ETA: 0s - loss: 1.8727 - mean_absolute_error: 0.7111- ETA: 1s - loss: 1.8711 - mean_ab\n",
      " Temp MAE = 0.268, Hum MAE = 1.114    \n",
      "9200/9200 [==============================] - 8s 846us/step - loss: 1.8736 - mean_absolute_error: 0.7113 - val_loss: 1.8345 - val_mean_absolute_error: 0.6911\n",
      "Epoch 22/50\n",
      "9170/9200 [============================>.] - ETA: 0s - loss: 1.8790 - mean_absolute_error: 0.7118\n",
      " Temp MAE = 0.237, Hum MAE = 1.092    \n",
      "9200/9200 [==============================] - 8s 910us/step - loss: 1.8790 - mean_absolute_error: 0.7120 - val_loss: 1.8361 - val_mean_absolute_error: 0.6644\n",
      "Epoch 23/50\n",
      "9122/9200 [============================>.] - ETA: 0s - loss: 1.8701 - mean_absolute_error: 0.7098- ETA: 2s - loss: 1.8721  - ETA: 1s - loss: 1.8621 - mean_\n",
      " Temp MAE = 0.266, Hum MAE = 1.248    \n",
      "9200/9200 [==============================] - 9s 932us/step - loss: 1.8742 - mean_absolute_error: 0.7108 - val_loss: 1.9804 - val_mean_absolute_error: 0.7574\n",
      "Epoch 24/50\n",
      "9187/9200 [============================>.] - ETA: 0s - loss: 1.8794 - mean_absolute_error: 0.7129\n",
      " Temp MAE = 0.262, Hum MAE = 1.241    \n",
      "9200/9200 [==============================] - 8s 873us/step - loss: 1.8803 - mean_absolute_error: 0.7131 - val_loss: 1.9341 - val_mean_absolute_error: 0.7515\n",
      "Epoch 25/50\n",
      "9167/9200 [============================>.] - ETA: 0s - loss: 1.8755 - mean_absolute_error: 0.7116- ETA: 1s - loss: 1.8740 - mean_\n",
      " Temp MAE = 0.238, Hum MAE = 1.110    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 10s 1ms/step - loss: 1.8764 - mean_absolute_error: 0.7120 - val_loss: 1.8268 - val_mean_absolute_error: 0.6738\n",
      "Epoch 26/50\n",
      "9160/9200 [============================>.] - ETA: 0s - loss: 1.8753 - mean_absolute_error: 0.7109- ET\n",
      " Temp MAE = 0.262, Hum MAE = 1.149    \n",
      "9200/9200 [==============================] - 9s 928us/step - loss: 1.8763 - mean_absolute_error: 0.7114 - val_loss: 1.8479 - val_mean_absolute_error: 0.7057\n",
      "Epoch 27/50\n",
      "9177/9200 [============================>.] - ETA: 0s - loss: 1.8735 - mean_absolute_error: 0.7104\n",
      " Temp MAE = 0.255, Hum MAE = 1.161    \n",
      "9200/9200 [==============================] - 9s 932us/step - loss: 1.8743 - mean_absolute_error: 0.7106 - val_loss: 1.8687 - val_mean_absolute_error: 0.7080\n",
      "Epoch 28/50\n",
      "9166/9200 [============================>.] - ETA: 0s - loss: 1.8674 - mean_absolute_error: 0.7089- ETA: 2s - loss: 1.9171 - ETA: 0s - loss: 1.8294 - mean_absolute_ - ETA: 0s - loss: 1.8703 - mean_absolute_error: 0.7097\n",
      " Temp MAE = 0.256, Hum MAE = 1.167    \n",
      "9200/9200 [==============================] - 9s 927us/step - loss: 1.8725 - mean_absolute_error: 0.7102 - val_loss: 1.8690 - val_mean_absolute_error: 0.7116\n",
      "Epoch 29/50\n",
      "9157/9200 [============================>.] - ETA: 0s - loss: 1.8713 - mean_absolute_error: 0.7097- ETA: 6s - loss: 1.5736 - mean_absolute_err - ETA: 6s - loss: 2.2668 - mean_absolute_ - ETA: 5s - loss: 1.8170 - mean_absolute_error: 0.6 - ETA: 5s - loss: 1.8 - ETA: 3 - ETA: 1s - loss: 1.8359 - - ETA: 0s - loss: 1.8412 - mean_absolute_error: 0\n",
      " Temp MAE = 0.279, Hum MAE = 1.185    \n",
      "9200/9200 [==============================] - 9s 940us/step - loss: 1.8732 - mean_absolute_error: 0.7102 - val_loss: 1.9044 - val_mean_absolute_error: 0.7318\n",
      "Epoch 30/50\n",
      "9159/9200 [============================>.] - ETA: 0s - loss: 1.8679 - mean_absolute_error: 0.7093\n",
      " Temp MAE = 0.277, Hum MAE = 1.112    \n",
      "9200/9200 [==============================] - 9s 928us/step - loss: 1.8732 - mean_absolute_error: 0.7105 - val_loss: 1.8419 - val_mean_absolute_error: 0.6944\n",
      "Epoch 31/50\n",
      "9139/9200 [============================>.] - ETA: 0s - loss: 1.8718 - mean_absolute_error: 0.7094- ETA: 2s - loss: 1.9234 - mean_absolute_err - ETA: 1s - loss: 1.8487 - mean_absolute_error: 0.703 - ETA: 1s - loss: 1.8421 - mean_absolute_error:  - ETA: 1s - loss: 1.8408 -\n",
      " Temp MAE = 0.243, Hum MAE = 1.142    \n",
      "9200/9200 [==============================] - 8s 917us/step - loss: 1.8748 - mean_absolute_error: 0.7103 - val_loss: 1.8851 - val_mean_absolute_error: 0.6926\n",
      "Epoch 32/50\n",
      "9166/9200 [============================>.] - ETA: 0s - loss: 1.8689 - mean_absolute_error: 0.7083\n",
      " Temp MAE = 0.254, Hum MAE = 1.100    \n",
      "9200/9200 [==============================] - 9s 943us/step - loss: 1.8717 - mean_absolute_error: 0.7090 - val_loss: 1.8288 - val_mean_absolute_error: 0.6771\n",
      "Epoch 33/50\n",
      "9170/9200 [============================>.] - ETA: 0s - loss: 1.8738 - mean_absolute_error: 0.7098\n",
      " Temp MAE = 0.248, Hum MAE = 1.109    \n",
      "9200/9200 [==============================] - 9s 1ms/step - loss: 1.8739 - mean_absolute_error: 0.7100 - val_loss: 1.8270 - val_mean_absolute_error: 0.6788\n",
      "Epoch 34/50\n",
      "9169/9200 [============================>.] - ETA: 0s - loss: 1.8696 - mean_absolute_error: 0.7080\n",
      " Temp MAE = 0.235, Hum MAE = 1.108    \n",
      "9200/9200 [==============================] - 8s 843us/step - loss: 1.8712 - mean_absolute_error: 0.7085 - val_loss: 1.8362 - val_mean_absolute_error: 0.6715\n",
      "Epoch 35/50\n",
      "9182/9200 [============================>.] - ETA: 0s - loss: 1.8695 - mean_absolute_error: 0.7080\n",
      " Temp MAE = 0.249, Hum MAE = 1.144    \n",
      "9200/9200 [==============================] - 8s 893us/step - loss: 1.8699 - mean_absolute_error: 0.7083 - val_loss: 1.9139 - val_mean_absolute_error: 0.6961\n",
      "Epoch 36/50\n",
      "9169/9200 [============================>.] - ETA: 0s - loss: 1.8702 - mean_absolute_error: 0.7088- ETA: 1s - loss: 1\n",
      " Temp MAE = 0.242, Hum MAE = 1.105    \n",
      "INFO:tensorflow:Assets written to: ./callback_mlp_V( a_alpha=0.03 )_chkp\\mlp_V( a_alpha=0.03 )_chkp_best\\assets\n",
      "9200/9200 [==============================] - 10s 1ms/step - loss: 1.8715 - mean_absolute_error: 0.7092 - val_loss: 1.8186 - val_mean_absolute_error: 0.6734\n",
      "Epoch 37/50\n",
      "9187/9200 [============================>.] - ETA: 0s - loss: 1.8705 - mean_absolute_error: 0.7084- ETA: 1s - loss: 1.8515 - mea\n",
      " Temp MAE = 0.249, Hum MAE = 1.168    \n",
      "9200/9200 [==============================] - 8s 903us/step - loss: 1.8703 - mean_absolute_error: 0.7085 - val_loss: 1.8735 - val_mean_absolute_error: 0.7083\n",
      "Epoch 38/50\n",
      "9171/9200 [============================>.] - ETA: 0s - loss: 1.8660 - mean_absolute_error: 0.7073\n",
      " Temp MAE = 0.248, Hum MAE = 1.110    \n",
      "9200/9200 [==============================] - 10s 1ms/step - loss: 1.8682 - mean_absolute_error: 0.7078 - val_loss: 1.8318 - val_mean_absolute_error: 0.6793\n",
      "Epoch 39/50\n",
      "9171/9200 [============================>.] - ETA: 0s - loss: 1.8662 - mean_absolute_error: 0.7075\n",
      " Temp MAE = 0.243, Hum MAE = 1.164    \n",
      "9200/9200 [==============================] - 10s 1ms/step - loss: 1.8700 - mean_absolute_error: 0.7083 - val_loss: 1.8861 - val_mean_absolute_error: 0.7034\n",
      "Epoch 40/50\n",
      "9164/9200 [============================>.] - ETA: 0s - loss: 1.8666 - mean_absolute_error: 0.7066\n",
      " Temp MAE = 0.254, Hum MAE = 1.142    \n",
      "9200/9200 [==============================] - 10s 1ms/step - loss: 1.8676 - mean_absolute_error: 0.7071 - val_loss: 1.8485 - val_mean_absolute_error: 0.6983\n",
      "Epoch 41/50\n",
      "9181/9200 [============================>.] - ETA: 0s - loss: 1.8674 - mean_absolute_error: 0.7083\n",
      " Temp MAE = 0.261, Hum MAE = 1.104    \n",
      "9200/9200 [==============================] - 9s 1ms/step - loss: 1.8705 - mean_absolute_error: 0.7090 - val_loss: 1.8301 - val_mean_absolute_error: 0.6826\n",
      "Epoch 42/50\n",
      "9113/9200 [============================>.] - ETA: 0s - loss: 1.8644 - mean_absolute_error: 0.7063\n",
      " Temp MAE = 0.259, Hum MAE = 1.106    \n",
      "9200/9200 [==============================] - 8s 877us/step - loss: 1.8704 - mean_absolute_error: 0.7079 - val_loss: 1.8288 - val_mean_absolute_error: 0.6824\n",
      "Epoch 43/50\n",
      "9177/9200 [============================>.] - ETA: 0s - loss: 1.8652 - mean_absolute_error: 0.7064\n",
      " Temp MAE = 0.288, Hum MAE = 1.247    \n",
      "9200/9200 [==============================] - 8s 917us/step - loss: 1.8669 - mean_absolute_error: 0.7069 - val_loss: 1.9450 - val_mean_absolute_error: 0.7672\n",
      "Epoch 44/50\n",
      "9150/9200 [============================>.] - ETA: 0s - loss: 1.8651 - mean_absolute_error: 0.7065\n",
      " Temp MAE = 0.271, Hum MAE = 1.112    \n",
      "9200/9200 [==============================] - 8s 880us/step - loss: 1.8670 - mean_absolute_error: 0.7072 - val_loss: 1.8428 - val_mean_absolute_error: 0.6919\n",
      "Epoch 45/50\n",
      "9195/9200 [============================>.] - ETA: 0s - loss: 1.8672 - mean_absolute_error: 0.7069\n",
      " Temp MAE = 0.239, Hum MAE = 1.130    \n",
      "9200/9200 [==============================] - 9s 926us/step - loss: 1.8675 - mean_absolute_error: 0.7070 - val_loss: 1.8901 - val_mean_absolute_error: 0.6849\n",
      "Epoch 46/50\n",
      "9158/9200 [============================>.] - ETA: 0s - loss: 1.8619 - mean_absolute_error: 0.7064\n",
      " Temp MAE = 0.236, Hum MAE = 1.152    \n",
      "9200/9200 [==============================] - 9s 960us/step - loss: 1.8673 - mean_absolute_error: 0.7070 - val_loss: 1.9163 - val_mean_absolute_error: 0.6940\n",
      "Epoch 47/50\n",
      "9167/9200 [============================>.] - ETA: 0s - loss: 1.8693 - mean_absolute_error: 0.7075\n",
      " Temp MAE = 0.232, Hum MAE = 1.114    \n",
      "9200/9200 [==============================] - 9s 959us/step - loss: 1.8702 - mean_absolute_error: 0.7077 - val_loss: 1.8588 - val_mean_absolute_error: 0.6728\n",
      "Epoch 48/50\n",
      "9132/9200 [============================>.] - ETA: 0s - loss: 1.8591 - mean_absolute_error: 0.7049\n",
      " Temp MAE = 0.260, Hum MAE = 1.196    \n",
      "9200/9200 [==============================] - 8s 878us/step - loss: 1.8665 - mean_absolute_error: 0.7065 - val_loss: 1.8914 - val_mean_absolute_error: 0.7280\n",
      "Epoch 49/50\n",
      "9187/9200 [============================>.] - ETA: 0s - loss: 1.8664 - mean_absolute_error: 0.7074\n",
      " Temp MAE = 0.252, Hum MAE = 1.095    \n",
      "9200/9200 [==============================] - 8s 872us/step - loss: 1.8672 - mean_absolute_error: 0.7075 - val_loss: 1.8424 - val_mean_absolute_error: 0.6737\n",
      "Epoch 50/50\n",
      "9170/9200 [============================>.] - ETA: 0s - loss: 1.8629 - mean_absolute_error: 0.7055\n",
      " Temp MAE = 0.298, Hum MAE = 1.164    \n",
      "9200/9200 [==============================] - 8s 851us/step - loss: 1.8631 - mean_absolute_error: 0.7058 - val_loss: 1.8866 - val_mean_absolute_error: 0.7311\n"
     ]
    }
   ],
   "source": [
    "if Structured == True :\n",
    "    model = get_model(mymodel)\n",
    "    history = model.fit(train_ds, epochs=50,   validation_data=val_ds,callbacks=[mycallback ,cp_callback ])\n",
    "if Structured == False :\n",
    "# Create  Unstructiured Pruning Callback  to to apply pruning during Training and fitting the model\n",
    "    model = MODELS[mymodel]\n",
    "    # Define the sparsity scheduler\n",
    "    pruning_params = {'pruning_schedule':\n",
    "    tfmot.sparsity.keras.PolynomialDecay(\n",
    "    initial_sparsity=0.30,\n",
    "    final_sparsity=sparsity,\n",
    "    begin_step=len(train_ds)*5,\n",
    "    end_step=len(train_ds)*15)\n",
    "    }\n",
    "\n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "    model = prune_low_magnitude(model, **pruning_params)\n",
    "    # Define the pruning callback\n",
    "    PruningCallback = [tfmot.sparsity.keras.UpdatePruningStep()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378e367-0524-432d-884e-462f68506e30",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Trianing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54f8b813-4f04-45e6-82bf-60cea919d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Flatten (Flatten)            (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 3)                 39        \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "Output_layes (Dense)         (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 3, 2)              0         \n",
      "=================================================================\n",
      "Total params: 75\n",
      "Trainable params: 75\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if Structured == False :\n",
    "    loss =   tf.keras.losses.MeanSquaredError()                       \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    metrics = [MultiOutputMAE()]\n",
    "    input_shape = [32, 6, 2]\n",
    "    model.build(input_shape)\n",
    "    model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "    history = model.fit(train_ds, epochs=50,   validation_data=val_ds,callbacks=[ PruningCallback])\n",
    "\n",
    "    model_to_export = tfmot.sparsity.keras.strip_pruning(model)\n",
    "\n",
    "    \n",
    "############################## Print Model Summary ####################\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93762cc8-06f7-4029-9081-3e31a198233c",
   "metadata": {},
   "source": [
    "# print the training results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "917fddf2-35b8-4c09-bb07-12114e8d7e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(mymodel+\".png\")\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f97423-df8d-4ad9-8cfa-5e54a922dc23",
   "metadata": {},
   "source": [
    "# Evaluation Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a18549d-52fb-4f21-8e93-fe9909cef292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pruned_Model_evaluate_and_compress_to_TFlite(model = model , model_to_export=None,tflite_model_dir =  TFLITE , model_name = mymodel  ):\n",
    "    \n",
    "    if not os.path.exists('./Pruned_models'):\n",
    "        os.makedirs('./Pruned_models')\n",
    "    # Model Evaluation\n",
    "    loss, error = model.evaluate(test_ds)\n",
    "    print(f'Temp mae = {error[0]:.3f}: , HUM mae = {error[1]:.3f} ')  \n",
    "    # Convert to TF lite without Quantization \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()  \n",
    "    # Write the model in binary formate and save it \n",
    "    with open(tflite_model_dir, 'wb') as fp:\n",
    "        fp.write(tflite_model)\n",
    "    Compressed =  f\"compressed_{TFLITE}\"\n",
    "    with open(Compressed, 'wb') as fp:\n",
    "        tflite_compressed = zlib.compress(tflite_model)\n",
    "        fp.write(tflite_compressed)\n",
    "    print(f\"the model is saved successfuly to {tflite_model_dir}\")\n",
    "    return Compressed , tflite_model_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3311768-d3fa-49db-bbb0-31084ee1829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_Quantization(tflite_model_dir =  TFLITE ,  PQT = False , WAPQT = False , Structured = Structured , saving_path = None ): \n",
    "    if Structured == False :\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    if Structured == True :\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(saving_path)\n",
    "    # Apply weight only quantization \n",
    "    if PQT == True :\n",
    "        tflite_model_dir = f\"PQT_{tflite_model_dir}\"\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "    # Apply weight + Activation  quantization \n",
    "    if WAPQT == True :\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        tflite_model_dir = f\"WAPQT_{tflite_model_dir}\"\n",
    "      \n",
    "    # Write the model in binary formate and save it \n",
    "    with open(tflite_model_dir, 'wb') as fp:\n",
    "        fp.write(tflite_model)\n",
    "    Compressed =  f\"compressed_{tflite_model_dir}\"\n",
    "    with open(Compressed, 'wb') as fp:\n",
    "        tflite_compressed = zlib.compress(tflite_model)\n",
    "        fp.write(tflite_compressed)\n",
    "    print(f\"the model is saved successfuly to {tflite_model_dir}\")\n",
    "    return Compressed , tflite_model_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4be46230-65a6-43ba-a951-60d3c3d94e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for weight and activations quantization \n",
    "def representative_dataset_gen():\n",
    "    for x, _ in train_ds.take(1000):\n",
    "        yield [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73617dec-ae71-491e-a495-7ab67e3b56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def S_pruning_Model_evaluate_and_compress_to_TFlite( tflite_model_dir =  TFLITE , chk_path = chk_path , model_name = mymodel , PQT = False , WAPQT = False , Structured = Structured):\n",
    "    if not os.path.exists('./models'):\n",
    "        os.makedirs('./models')\n",
    "    model = tf.keras.models.load_model(filepath = chk_path , custom_objects={'MultiOutputMAE':MultiOutputMAE})\n",
    "\n",
    "    run_model = tf.function(lambda x: model(x))\n",
    "    # input_shape = model.inputs[0].shape.as_list()\n",
    "    # input_shape[0] = batch_size\n",
    "    # func = tf.function(model).get_concrete_function(\n",
    "    # tf.TensorSpec(input_shape, model.inputs[0].dtype))\n",
    "    concrete_func = run_model.get_concrete_function(tf.TensorSpec([1, 6, 2], tf.float32))\n",
    "    saving_path = os.path.join('.','models', model_name)\n",
    "    model.save(saving_path ,signatures=concrete_func )\n",
    "\n",
    "    best_model = tf.keras.models.load_model(filepath = saving_path , custom_objects={'MultiOutputMAE':MultiOutputMAE})\n",
    "    loss, error = best_model.evaluate(test_ds)\n",
    "    print(f'Temp mae = {error[0]:.3f}: , HUM mae = {error[1]:.3f} ')\n",
    "    # Convert to TF lite without Quantization \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()  \n",
    "\n",
    "    # Write the model in binary formate and save it \n",
    "    with open(tflite_model_dir, 'wb') as fp:\n",
    "        fp.write(tflite_model)\n",
    "    Compressed = \"compressed_\"+tflite_model_dir \n",
    "    with open(Compressed, 'wb') as fp:\n",
    "        tflite_compressed = zlib.compress(tflite_model)\n",
    "        fp.write(tflite_compressed)\n",
    "    print(f\"the model is saved successfuly to {tflite_model_dir}\")\n",
    "    return Compressed , tflite_model_dir , saving_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53d6e3-2dda-41cc-b5a7-ab26d8c55d75",
   "metadata": {},
   "source": [
    "# load_and_evaluation of TF lite model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19d4db77-169e-4e1f-9947-c41ada35c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluation(path, dataset):\n",
    "    interpreter = tf.lite.Interpreter(model_path = path) \n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    dataset = test_ds.unbatch().batch(1)\n",
    "\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    print(dataset)\n",
    "\n",
    "    for data in dataset:\n",
    "        my_input = np.array(data[0], dtype = np.float32)\n",
    "        label = np.array(data[1], dtype = np.float32)\n",
    "        # print (f\"my_input = {my_input}\")\n",
    "        # print(f\"label = {label}\")\n",
    "\n",
    "    \n",
    "            \n",
    "        labels.append(label)\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], my_input)\n",
    "        interpreter.invoke()\n",
    "        my_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        outputs.append(my_output[0])\n",
    "\n",
    "    outputs = np.squeeze( np.array(outputs))\n",
    "    labels = np.squeeze(np.array(labels))\n",
    "\n",
    "    \n",
    "    error = np.absolute(outputs - labels)\n",
    "  \n",
    "    mean_axis_1 = np.mean(error , axis = 1)     #  ==>  np.sum(error, axis = 1)/labels.shape[1]\n",
    "    \n",
    "    mae = np.mean(mean_axis_1 , axis = 0)  #  ==> np.sum(mean_axis_1, axis = 0) /mean_axis_1.shape[0]\n",
    "    temp_MAE = mae[0] \n",
    "    hum_MAE = mae[1]\n",
    "    print(\"*\"*50,\"\\n\",f\" Excuting the model {path} \")\n",
    "    print(\"*\"*50,\"\\n\",f\" mae is {mae} of shape {mae.shape} \")\n",
    "    print(\"*\"*50,\"\\n\",f'Temp mae = {mae[0]:.3f}: , HUM mae = {mae[1]:.3f} ')\n",
    "    \n",
    "    # check version \"a\" requirment\n",
    "    if version == \"a\" :\n",
    "        if temp_MAE <= 0.3 and hum_MAE <= 1.2 :\n",
    "            print (\"*\"*50,\"\\n\",\"achieved the requirments \" )\n",
    "        else :\n",
    "            print (\"*\"*50,\"\\n\",\"Not achieved the requirments\" )\n",
    "    # check version \"b\" requirment        \n",
    "    if version == \"b\" :\n",
    "        if temp_MAE <= 0.7 and hum_MAE <= 2.5 :\n",
    "            print (\"*\"*50,\"\\n\",\"achieved the requirments \" )\n",
    "        else :\n",
    "            print (\"*\"*50,\"\\n\",\"Not achieved the requirments\" ) \n",
    "\n",
    "#     return mae \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ec949-94cf-4ce3-a393-395fe8072468",
   "metadata": {},
   "source": [
    "# get the size of tf_lite model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2daddeb-9bda-4f53-8239-f2761035d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsize(file):\n",
    "    st = os.stat(file)\n",
    "    size = st.st_size\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d5f4c-dc32-4b77-940b-8cf2a23c4625",
   "metadata": {},
   "source": [
    "### Without Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42b077dc-6ec5-46f8-83ad-80f2b9235d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\models\\mlp_V( a_alpha=0.03 )\\assets\n",
      "1314/1314 [==============================] - 2s 2ms/step - loss: 1.9492 - mean_absolute_error: 0.7225\n",
      "Temp mae = 0.259: , HUM mae = 1.186 \n",
      "INFO:tensorflow:Assets written to: C:\\Users\\musta\\AppData\\Local\\Temp\\tmp3hef7ib8\\assets\n",
      "the model is saved successfuly to mlp_V( a_alpha=0.03 ).tflite\n",
      " \n",
      " Size of TF.lite model is 3.128 KB\n",
      " \n",
      " Size of compressed_tflite model is 1.454 KB\n",
      "mlp_V( a_alpha=0.03 ).tflite\n"
     ]
    }
   ],
   "source": [
    "if Structured == False :\n",
    "    tflite_model_dir_Compressed ,tf_lite_model_path = Pruned_Model_evaluate_and_compress_to_TFlite(model_to_export=model_to_export)\n",
    "if Structured == True :\n",
    "    tflite_model_dir_Compressed ,tf_lite_model_path, saving_path  = S_pruning_Model_evaluate_and_compress_to_TFlite(TFLITE)\n",
    "# print(b)\n",
    "tflite_size = getsize(tf_lite_model_path)\n",
    "compressed_tflite_size = getsize(tflite_model_dir_Compressed )\n",
    "print(f\" \\n Size of TF.lite model is {tflite_size /1000} KB\")\n",
    "print(f\" \\n Size of compressed_tflite model is {compressed_tflite_size /1000} KB\")\n",
    "print(tf_lite_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a006e396-b68d-4369-846a-171b71853b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 6, 2), (None, 3, 2)), types: (tf.float32, tf.float32)>\n",
      "************************************************** \n",
      "  Excuting the model mlp_V( a_alpha=0.03 ).tflite \n",
      "************************************************** \n",
      "  mae is [0.258553  1.1863916] of shape (2,) \n",
      "************************************************** \n",
      " Temp mae = 0.259: , HUM mae = 1.186 \n",
      "************************************************** \n",
      " achieved the requirments \n"
     ]
    }
   ],
   "source": [
    "load_and_evaluation(tf_lite_model_path , test_ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f0192-e2be-4650-9d8f-0f4ffa78a661",
   "metadata": {},
   "source": [
    "### Weights only Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae31c5a4-b8db-46e0-9a55-fcd879217924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is saved successfuly to PQT_mlp_V( a_alpha=0.03 ).tflite\n",
      " \n",
      " Size of TF.lite model is 4.08 KB\n",
      " \n",
      " Size of compressed_tflite model is 1.529 KB\n"
     ]
    }
   ],
   "source": [
    "if Structured == False :\n",
    "    tflite_model_dir_Compressed ,tf_lite_model_path = apply_Quantization( PQT=True)\n",
    "if Structured == True :\n",
    "    Compressed , Quantized   = apply_Quantization(PQT=True ,  saving_path=chk_path)\n",
    "# print(b)\n",
    "tflite_size = getsize(Quantized)\n",
    "compressed_tflite_size = getsize(Compressed )\n",
    "print(f\" \\n Size of TF.lite model is {tflite_size /1000} KB\")\n",
    "print(f\" \\n Size of compressed_tflite model is {compressed_tflite_size /1000} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f3003aa-e255-4aec-895c-81a55b443518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 6, 2), (None, 3, 2)), types: (tf.float32, tf.float32)>\n",
      "************************************************** \n",
      "  Excuting the model PQT_mlp_V( a_alpha=0.03 ).tflite \n",
      "************************************************** \n",
      "  mae is [0.258553  1.1863916] of shape (2,) \n",
      "************************************************** \n",
      " Temp mae = 0.259: , HUM mae = 1.186 \n",
      "************************************************** \n",
      " achieved the requirments \n"
     ]
    }
   ],
   "source": [
    "load_and_evaluation(Quantized , test_ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191d4b6-d449-465a-adc5-b4367a60027f",
   "metadata": {},
   "source": [
    "###  Weights + activations  Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1b23043-2634-4a5c-af7e-937d374ca918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is saved successfuly to WAPQT_mlp_V( a_alpha=0.03 ).tflite\n",
      " \n",
      " Size of TF.lite model is 4.816 KB\n",
      " \n",
      " Size of compressed_tflite model is 1.598 KB\n"
     ]
    }
   ],
   "source": [
    "if Structured == False :\n",
    "    tflite_model_dir_Compressed ,tf_lite_model_path = apply_Quantization( WAPQT=True)\n",
    "if Structured == True :\n",
    "    Compressed , Quantized   = apply_Quantization(WAPQT=True , saving_path=chk_path)\n",
    "# print(b)\n",
    "tflite_size = getsize(Quantized)\n",
    "compressed_tflite_size = getsize(Compressed )\n",
    "print(f\" \\n Size of TF.lite model is {tflite_size /1000} KB\")\n",
    "print(f\" \\n Size of compressed_tflite model is {compressed_tflite_size /1000} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0251fb8c-7814-4052-8256-0c71406e9274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 6, 2), (None, 3, 2)), types: (tf.float32, tf.float32)>\n",
      "************************************************** \n",
      "  Excuting the model WAPQT_mlp_V( a_alpha=0.03 ).tflite \n",
      "************************************************** \n",
      "  mae is [0.38221422 1.216565  ] of shape (2,) \n",
      "************************************************** \n",
      " Temp mae = 0.382: , HUM mae = 1.217 \n",
      "************************************************** \n",
      " Not achieved the requirments\n"
     ]
    }
   ],
   "source": [
    "load_and_evaluation(Quantized , test_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af99f9-2046-4156-92e5-3890e783f925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, required=True, help='model name')\n",
    "# parser.add_argument('--labels', type=int, required=True, help='model output')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True,\n",
    "    cache_dir='.', cache_subdir='data')\n",
    "csv_path, _ = os.path.splitext(zip_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "column_indices = [2, 5]\n",
    "columns = df.columns[column_indices]\n",
    "data = df[columns].values.astype(np.float32)\n",
    "\n",
    "n = len(data)\n",
    "train_data = data[0:int(n*0.7)]\n",
    "val_data = data[int(n*0.7):int(n*0.9)]\n",
    "test_data = data[int(n*0.9):]\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2630fca0-5061-4117-a6d9-1a36ca4a4e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print (mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2997a3e-9d6e-49c8-bb17-6469e0602568",
   "metadata": {},
   "source": [
    "# WindowGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f0113e-bd14-4dd6-9872-c045adf11913",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = 6\n",
    "version = \"b\"                     ####### Remove this later\n",
    "if version == \"a\" :               ####### Remember this is the argparse Arument --version \n",
    "    output_steps = 3\n",
    "if version == \"b\" :\n",
    "    output_steps = 9\n",
    "\n",
    "\n",
    "class WindowGenerator:\n",
    "    def __init__(self, input_width, output_steps, mean, std):\n",
    "        self.input_width = input_width\n",
    "        self.output_steps = output_steps\n",
    "        self.mean = tf.reshape(tf.convert_to_tensor(mean), [1, 1, 2])\n",
    "        self.std = tf.reshape(tf.convert_to_tensor(std), [1, 1, 2])\n",
    "\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, :self.input_width, :]        # for example if total window size = 9 input =  [:,:6 ,:] --> output [:,-3: , ] outpu_tstep = 3 \n",
    "        labels = features[:, -self.output_steps :, :]\n",
    "\n",
    "        inputs.set_shape([None, self.input_width, 2])\n",
    "        labels.set_shape([None, self.output_steps,2])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def normalize(self, features):\n",
    "        features = (features - self.mean) / (self.std + 1.e-6)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def preprocess(self, features):\n",
    "        inputs, labels = self.split_window(features)\n",
    "        inputs = self.normalize(inputs)\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data, train):\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "                data=data,\n",
    "                targets=None,\n",
    "                sequence_length = input_width + self.output_steps,                       #### this change because now the total depends on the output widht\n",
    "                sequence_stride = 1,\n",
    "                batch_size = 32)\n",
    "        ds = ds.map(self.preprocess)\n",
    "        ds = ds.cache()\n",
    "        if train is True:\n",
    "            ds = ds.shuffle(100, reshuffle_each_iteration=True)\n",
    "\n",
    "        return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669e1a5f-8f69-45c8-8113-a5f9d2c58ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = np.arange(18)\n",
    "# window = window.reshape(9,2)\n",
    "# print(len(window))\n",
    "# print(window)\n",
    "# print(window[:6].shape)\n",
    "# print(window[-3:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0494564-1658-4fef-954a-c68d8e43aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = WindowGenerator(input_width, output_steps, mean, std)\n",
    "train_ds = generator.make_dataset(train_data, True)\n",
    "val_ds = generator.make_dataset(val_data, False)\n",
    "test_ds = generator.make_dataset(test_data, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df990243-6f6c-423c-82ac-48394aaba1e0",
   "metadata": {},
   "source": [
    "# checking the shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43459a86-1f13-437e-97cb-d69cf31dd1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape before split (420551, 2)\n",
      "train_data (294385, 2)\n",
      "val_data (84110, 2)\n",
      "test_data (42056, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"data shape before split {data.shape}\")\n",
    "\n",
    "print(f\"train_data {train_data.shape}\")\n",
    "\n",
    "print(f\"val_data {val_data.shape}\")\n",
    "\n",
    "print(f\"test_data {test_data.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b7674be-c965-434b-b755-6b580a570ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb242899-5272-42fa-8bbd-8a7549fc3e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 6, 2)\n",
      "(32, 9, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inp , label = next(it)\n",
    "print(inp.shape)\n",
    "print(label.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fec2d-cfbf-4d4b-83d1-c79d9990dfc1",
   "metadata": {},
   "source": [
    "# MultiOutputMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04dbeda9-86ed-4738-8ef3-00e95ef20e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exercise 1.7\n",
    "class MultiOutputMAE(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='mean_absolute_error', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total = self.add_weight('total', initializer='zeros', shape=(2,))\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        error = tf.abs(y_pred- y_true)     \n",
    "        error = tf.reduce_mean(error, axis=[0,1])  # compute the mean for all samples in the batch for each feature (temp , hum) ==> output shape = (2,)\n",
    "        self.total.assign_add(error)\n",
    "        self.count.assign_add(1.)\n",
    "\n",
    "        return\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.count.assign(tf.zeros_like(self.count))\n",
    "        self.total.assign(tf.zeros_like(self.total))\n",
    "\n",
    "    def result(self):\n",
    "        result = tf.math.divide_no_nan(self.total, self.count)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf89bc-9183-41d8-8b5a-5cbd15171774",
   "metadata": {},
   "source": [
    "# building the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10a7f2c8-643d-4fa1-9d3a-3f6ac383907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.3\n",
    "mlp = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (input_width,2) , name='Flatten'),\n",
    "        tf.keras.layers.Dense(128, activation='relu' , name='Dense1'),\n",
    "        tf.keras.layers.Dense(128, activation='relu' , name='Dense2'),\n",
    "        tf.keras.layers.Dense(units = 2*output_steps , name='Output_layes'), \n",
    "        tf.keras.layers.Reshape([output_steps, 2])\n",
    "    ])\n",
    "\n",
    "############################################################################\n",
    "cnn = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(input_shape = (input_width,2) , filters=64, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=2*output_steps), \n",
    "        tf.keras.layers.Reshape([output_steps, 2])\n",
    "    ])\n",
    "\n",
    "############################################################################\n",
    "\n",
    "lstm = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(input_shape = (input_width,2) ,units=64),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=2*output_steps),\n",
    "        tf.keras.layers.Reshape([output_steps, 2]), \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79ec65fb-5e88-4598-b606-838136be3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {'mlp': mlp, 'cnn': cnn, 'lstm': lstm}\n",
    "mymodel = 'lstm'\n",
    "  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71ff7847-5c1f-4d91-b905-1df171f6139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pydot\n",
    "# ! pip install pydotplus\n",
    "# ! pip install graphviz\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cdb9b9d-ace7-4bf6-91cf-1a4c3d9cb842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "tf.keras.utils.plot_model(model, to_file=f'{mymodel}.png',show_layer_names=True , show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81720020-46c5-4fa7-ba23-dcb757909f79",
   "metadata": {},
   "source": [
    "# Define losses & Optimizer & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6111f00-342d-4f04-8309-5b160a9d7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model):\n",
    "    model = MODELS[mymodel]\n",
    "    loss =   tf.keras.losses.MeanSquaredError()                       #tf.keras.losses.MeanSquaredError()\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    metrics = [MultiOutputMAE()]\n",
    "\n",
    "    # Training and optimizing\n",
    "\n",
    "    model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53d09677-3b03-4ad3-ad0a-1565915ebeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEMP_HUM_VAL(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        hum = logs[\"val_mean_absolute_error\"][1]\n",
    "        temp = logs[\"val_mean_absolute_error\"][0]\n",
    "        print(f\"\\n Hum MAE = {hum} , Temp MAE = {temp} \")\n",
    "        # return temp , hum\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54f8b813-4f04-45e6-82bf-60cea919d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "9195/9200 [============================>.] - ETA: 0s - loss: 6.2245 - mean_absolute_error: 1.3413\n",
      " Hum MAE = 2.1311137676239014 , Temp MAE = 0.6020656824111938 \n",
      "9200/9200 [==============================] - 18s 2ms/step - loss: 6.2237 - mean_absolute_error: 1.3413 - val_loss: 6.2131 - val_mean_absolute_error: 1.3666\n",
      "Epoch 2/2\n",
      "9186/9200 [============================>.] - ETA: 0s - loss: 6.1971 - mean_absolute_error: 1.3376\n",
      " Hum MAE = 2.2985217571258545 , Temp MAE = 0.6088789701461792 \n",
      "9200/9200 [==============================] - 18s 2ms/step - loss: 6.2064 - mean_absolute_error: 1.3384 - val_loss: 6.7111 - val_mean_absolute_error: 1.4537\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 64)                17152     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                1170      \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 9, 2)              0         \n",
      "=================================================================\n",
      "Total params: 18,322\n",
      "Trainable params: 18,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mycallback = TEMP_HUM_VAL()\n",
    "model = get_model(mymodel)\n",
    "# tb_run = 0\n",
    "# tb_callback = keras.callbacks.TensorBoard(log_dir='./tb_log/run_{}'.format(tb_run), histogram_freq=1)\n",
    "\n",
    "# history = model.fit(inp_train,target_train,  epochs=5,validation_data=val_ds ,batch_size=len(batch_train), callbacks=[tb_callback , model_checkpoint_callback] , verbose=2)\n",
    "history = model.fit(train_ds, epochs=20,   validation_data=val_ds,callbacks=[mycallback])\n",
    "# tb_run += 1\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee8c64c2-59a2-4d2c-95cb-e1fbe9fbabd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mean_absolute_error', 'val_loss', 'val_mean_absolute_error'])\n"
     ]
    }
   ],
   "source": [
    "x = history.history\n",
    "print(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e5f3dda-a665-44f6-8036-93515171b394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53423244 2.2134619 ]\n"
     ]
    }
   ],
   "source": [
    "print(x[\"val_mean_absolute_error\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f97423-df8d-4ad9-8cfa-5e54a922dc23",
   "metadata": {},
   "source": [
    "# Evaluation Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a2f7f3ef-61b9-4619-8712-6c1bfd54cc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1314/1314 [==============================] - 1s 872us/step - loss: 6.6840 - mean_absolute_error: 1.4643\n",
      "Temp mae = 0.5911256670951843: , HUM mae = 2.3374264240264893 \n"
     ]
    }
   ],
   "source": [
    "loss, error = model.evaluate(test_ds)\n",
    "print(f'Temp mae = {error[0]}: , HUM mae = {error[1]} ')\n",
    "# error[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f597d33-476e-42dd-a403-4a4c139f83bf",
   "metadata": {},
   "source": [
    "# TF LITE MODEL SIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73617dec-ae71-491e-a495-7ab67e3b56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_TF_lite(model_name, tflite_dic):\n",
    "    if not os.path.exists('./models'):\n",
    "        os.makedirs('./models')\n",
    "\n",
    "    run_model = tf.function(lambda x: model(x))\n",
    "    concrete_func = run_model.get_concrete_function(tf.TensorSpec([1, 6, 2], tf.float32))\n",
    "    saving_path = os.path.join('.','models', model_name)\n",
    "    model.save(saving_path ,signatures=concrete_func )\n",
    "\n",
    "    best_model = tf.keras.models.load_model(filepath = saving_path , custom_objects={'MultiOutputMAE':MultiOutputMAE})\n",
    "    best_model.evaluate(test_ds)\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(saving_path)\n",
    "    tflite_model = converter.convert()\n",
    "    tflite_model_dir = tflite_dic\n",
    "    with open(tflite_model_dir, 'wb') as fp:\n",
    "        fp.write(tflite_model)\n",
    "    print(f\"the model is saved to {tflite_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e32f8-d504-4a17-865f-d0670154cb7d",
   "metadata": {},
   "source": [
    "### USE THIS FUNCTION TO CREATE AND SAVE TF.LITE / REMEMBER TO CHANGE THE NAMES TO YOUR MODEL AND VERSION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bc627ba3-5c42-4f0d-a712-99d60291af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\models\\test\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\models\\test\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1314/1314 [==============================] - 1s 879us/step - loss: 6.6840 - mean_absolute_error: 1.4643\n",
      "the model is saved to tflite_lstm\n"
     ]
    }
   ],
   "source": [
    "save_TF_lite(\"test\" , \"test_tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a1206-470b-4e4d-a9e7-8be189085b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018e8f61-0a4e-4f63-9910-0dd611924af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreter = tf.lite.Interpreter(model_path='E:\\Github\\Machine-learning-for-IOT\\Lab3\\ex1\\CNN\\saved_model.pb')\n",
    "# interpreter.allocate_tensors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
